# docker-compose.dev.yml (FOR LOCAL DEVELOPMENT)
version: '3.8'

services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: seoul-flow-frontend-dev
    ports:
      - "${FE_PORT}:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    networks:
      - seoul-flow-net

  zookeeper:
    ports:
      - "${ZOOKEEPER_PORT}:2181"

  kafka:
    ports:
      - "${KAFKA_PORT}:${KAFKA_PORT}"
    environment:
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:${KAFKA_PORT}

  akhq:
    ports:
      - "${AKHQ_PORT}:8080"
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            docker-kafka-cluster:
              properties:
                bootstrap.servers: "kafka:29092"

  spark:
    build: ./spark
    container_name: spark-seoul-flow-dev
    depends_on:
      - kafka
    ports:
      # Spark UI에 접속하기 위한 포트. http://localhost:4040 으로 접속 가능
      - "4040:4040"
    user: root
    volumes:
      - spark_ivy_cache:/root/.ivy2
    networks:
      - seoul-flow-net
    # 개발용 카프카의 내부 리스너 주소(kafka:29092)를 인자로 전달
    command: >
      bash -c "
        echo 'Waiting for Kafka to be ready...'
        # nc (netcat)을 사용해 kafka:29092 포트가 열릴 때까지 30초간 대기
        count=0
        while ! nc -z kafka 29092 && [[ $$count -lt 30 ]]; do
          sleep 1
          count=$$((count+1))
        done
        echo 'Kafka is ready. Starting Spark application...'
        /opt/bitnami/spark/bin/spark-submit
        --class com.seoulflow.spark.GrazingDetector
        --master local[*]
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6
        /app/app.jar
      "

volumes:
  spark_ivy_cache: